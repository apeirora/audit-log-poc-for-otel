version: "3"

description: Taskfile for port-forwarding the otel-demo frontend-proxy service.

vars:
  WORK_DIR: "{{ .ROOT_DIR }}/work_dir"
  CLUSTER:
    sh: |
      if {{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1; then
        echo true
      else
        echo false
      fi
  NODE_IP:
    sh: |
      {{ if eq .CLUSTER "true" }} {{ .KUBECTL_CMD }} get nodes -o yaml | yq '.items[] | select(.metadata.name | test("otel-audit-log")) | .metadata.annotations."alpha.kubernetes.io/provided-node-ip"' {{ end }}
  COLLECTOR_HTTP:
    sh: |
      {{ if eq .CLUSTER "true" }} {{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get svc otel-collector | yq '.spec.ports[] | select(.name == "otlp-http") | .nodePort' {{ end }}
  COLLECTOR_GRPC:
    sh: |
      {{ if eq .CLUSTER "true" }} {{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get svc otel-collector | yq '.spec.ports[] | select(.name == "otlp") | .nodePort' {{ end }}
  DICE_PORT_GO:
    sh: |
      {{ if eq .CLUSTER "true" }} {{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get svc dice-go | yq '.spec.ports[] | .nodePort' {{ end }}
  DICE_PORT_JAVA:
    sh: |
      {{ if eq .CLUSTER "true" }} {{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get svc dice-java | yq '.spec.ports[] | .nodePort' {{ end }}

includes:
  tools:
    desc: Install required tools (k3d, kubectl, kind).
    taskfile: ./tools.yaml
    internal: true

tasks:
  demo-install:
    desc: Install otel-demo in the local cluster.
    run: once
    deps:
      - task: tools:install-helm
    requires:
      vars: [HELM_CMD]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1"
        msg: "Please ensure that you have a k8s-cluster running. Try:\ttask cluster:start"
    status:
      - "{{ .HELM_CMD }} list --namespace otel-demo -o json | jq -r '.[].status' | grep -q deployed"
    cmds:
      - "{{ .HELM_CMD }} repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts >/dev/null 2>&1"
      - "{{ .HELM_CMD }} install --values helm/otel-demo-overrides.yaml my-otel-demo open-telemetry/opentelemetry-demo --namespace otel-demo --create-namespace"

  demo-upgrade:
    desc: Upgrade otel-demo in the local cluster.
    run: once
    deps:
      - task: tools:install-helm
    requires:
      vars: [HELM_CMD]
    cmds:
      - "{{ .HELM_CMD }} upgrade --install --values helm/otel-demo-overrides.yaml my-otel-demo open-telemetry/opentelemetry-demo --namespace otel-demo --create-namespace"

  demo-status:
    desc: Check the status of otel-demo in the local cluster.
    deps:
      - task: demo-install
    requires:
      vars: [HELM_CMD]
    cmds:
      - "{{ .HELM_CMD }} status --namespace otel-demo my-otel-demo"

  demo-delete:
    desc: Delete otel-demo from the local cluster.
    deps:
      - task: tools:install-helm
    requires:
      vars: [HELM_CMD]
    status:
      - "! {{ .HELM_CMD }} list --namespace otel-demo | grep -q my-otel-demo"
    cmds:
      - "{{ .HELM_CMD }} uninstall my-otel-demo --namespace otel-demo --ignore-not-found"

  port-forward:
    desc: Port-forward the otel-demo frontend-proxy service to localhost:8080.
    deps:
      - task: demo-install
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} get svc/frontend-proxy --namespace otel-demo >/dev/null 2>&1"
      - sh: "{{ .KUBECTL_CMD }} get pods --namespace otel-demo --field-selector=status.phase=Running | grep -q frontend-proxy"
        msg: "Please wait for the frontend-proxy pod to be in Running state."
    cmds:
      - "{{ .KUBECTL_CMD }} --namespace otel-demo port-forward svc/frontend-proxy 8080:8080"

  tweak-config:
    desc: Tweak the otel-collector configuration in the otel-demo namespace.
    deps:
      - task: create-work-dir
      - task: demo-install
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD]
    cmds:
      - "{{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get configmap otel-collector > {{ .WORK_DIR }}/otel-collector-configmap.yaml"
      - "{{ .KUBECTL_CMD }} --namespace otel-demo --output yaml get service otel-collector > {{ .WORK_DIR }}/otel-collector-service.yaml"
      - yq eval '.data.relay = load_str("{{ .ROOT_DIR }}/otel-collector/config.yaml")' {{ .WORK_DIR }}/otel-collector-configmap.yaml > {{ .WORK_DIR }}/otel-collector-configmap-new.yaml
      - yq eval '.spec.ports |= map(select(.name == "otlp" or .name == "otlp-http"))' {{ .WORK_DIR }}/otel-collector-service.yaml > {{ .WORK_DIR }}/otel-collector-service-new.yaml
      - "{{ .KUBECTL_CMD }} --namespace otel-demo apply --filename {{ .WORK_DIR }}/otel-collector-configmap-new.yaml"
      - "{{ .KUBECTL_CMD }} --namespace otel-demo apply --filename {{ .WORK_DIR }}/otel-collector-service-new.yaml"
      - "{{ .KUBECTL_CMD }} --namespace otel-demo rollout restart deployment otel-collector"

  create-work-dir:
    desc: Create the work directory ({{ .WORK_DIR }}) if it doesn't exist.
    internal: true
    run: once
    platforms: [linux, darwin]
    requires:
      vars: [WORK_DIR]
    status:
      - test -d "{{ .WORK_DIR }}"
    cmds:
      - mkdir -p "{{ .WORK_DIR }}"

  busybox:
    desc: Run a busybox container in the otel-demo namespace.
    deps:
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD]
    cmds:
      - "{{ .KUBECTL_CMD }} run run -it --rm --restart=Never --namespace otel-demo --image=busybox -- sh"

  alpine-sidecar:
    desc: Run a
    deps:
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD]
    cmds:
      - "{{ .KUBECTL_CMD }} exec -it $({{ .KUBECTL_CMD }} get pods --namespace otel-demo --field-selector=status.phase=Running -o name | grep otel-collector) -c alpine-sidecar --namespace otel-demo -- /bin/sh"

  export-collector:
    desc: Shows the otel-collector service details (IP address and ports).
    requires:
      vars: [NODE_IP, COLLECTOR_GRPC, COLLECTOR_HTTP]
    cmds:
      - |
        echo export OTEL_EXPORTER_OTLP_ENDPOINT="http://{{ .NODE_IP }}" && \
        echo export OTEL_EXPORTER_OTLP_ENDPOINT_GRPC="http://{{ .NODE_IP }}:{{ .COLLECTOR_GRPC }}" && \
        echo export OTEL_EXPORTER_OTLP_ENDPOINT_HTTP="http://{{ .NODE_IP }}:{{ .COLLECTOR_HTTP }}"

  audit-log-sink:
    desc: Install audit-log-sink in the local cluster.
    run: once
    deps:
      - task: tools:install-helm
    requires:
      vars: [HELM_CMD]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1"
        msg: "Please ensure that you have a k8s-cluster running. Try:\ttask cluster:start"
    cmds:
      - "{{ .HELM_CMD }} repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts >/dev/null 2>&1"
      - "{{ .HELM_CMD }} install --values helm/audit-log-sink-overrides.yaml audit-log-sink open-telemetry/opentelemetry-collector --namespace otel-demo --create-namespace"

  simple-setup:
    desc: Install a simple setup of otel-collector and otel-collector-sink in the local cluster.
    run: once
    deps:
      - task: tools:install-kubectl
      - task: demo-delete
    requires:
      vars: [KUBECTL_CMD]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1"
        msg: "Please ensure that you have a k8s-cluster running. Try:\ttask cluster:start"
    cmds:
      - "{{ .HELM_CMD }} uninstall audit-log-sink --namespace otel-demo --ignore-not-found"
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/namespace.yaml"
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/log-sink.yaml"
      - "{{ .KUBECTL_CMD }} rollout restart deployment/log-sink -n otel-demo"
      - "{{ .KUBECTL_CMD }} wait --for=condition=available --timeout=120s deployment/log-sink -n otel-demo"
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/collector.yaml"
      - "{{ .KUBECTL_CMD }} rollout restart deployment/collector -n otel-demo"

  deploy-dice-go:
    desc: Deploy dice-go app in the local cluster.
    deps:
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD, NODE_IP, DICE_PORT_GO]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1"
        msg: "Please ensure that you have a k8s-cluster running. Try:\ttask cluster:start"
    cmds:
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/namespace.yaml"
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/dice-go.yaml"
      - "{{ .KUBECTL_CMD }} rollout restart deployment/dice-go -n otel-demo"
      - echo export DICE_URL_GO="http://{{ .NODE_IP }}:{{ .DICE_PORT_GO }}/rolldice/$USER"

  deploy-dice-java:
    desc: Deploy dice-java app in the local cluster.
    deps:
      - task: tools:install-kubectl
    requires:
      vars: [KUBECTL_CMD, NODE_IP, DICE_PORT_JAVA]
    preconditions:
      - sh: "{{ .KUBECTL_CMD }} cluster-info >/dev/null 2>&1"
        msg: "Please ensure that you have a k8s-cluster running. Try:\ttask cluster:start"
    cmds:
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/namespace.yaml"
      - "{{ .KUBECTL_CMD }} apply --filename kubectl/dice-java.yaml"
      - "{{ .KUBECTL_CMD }} rollout restart deployment/dice-java -n otel-demo"
      - echo export DICE_URL_JAVA="http://{{ .NODE_IP }}:{{ .DICE_PORT_JAVA }}/rolldice/$USER"

  throw-dice:
    desc: Throw a dice using the dice-go or dice-java app.
    requires:
      vars: [NODE_IP, DICE_PORT_GO, DICE_PORT_JAVA]
    deps:
      - task: deploy-dice-go
      - task: deploy-dice-java
    cmds:
      - curl -s "http://{{ .NODE_IP }}:{{ .DICE_PORT_GO }}/rolldice/$USER"
      - curl -s "http://{{ .NODE_IP }}:{{ .DICE_PORT_JAVA }}/rolldice/$USER"
